{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.6 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Study/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras import regularizers\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import atexit\n",
    "import ipywidgets as widgets\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "num_features = 10\n",
    "input_shape = (num_features,)\n",
    "\n",
    "\n",
    "input_data = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "hidden_layer = tf.keras.layers.Dense(units=64, activation=tf.nn.relu)(input_data)\n",
    "output_layer = tf.keras.layers.Dense(units=num_features, activation=None)(hidden_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "num_samples = 1000\n",
    "input_data = np.random.randn(num_samples, num_features)\n",
    "output_data = input_data\n",
    "\n",
    "model.fit(input_data, output_data, epochs=100, batch_size=32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = {'DeviceID':str, 'Fault_Code_Type_1':str, 'Fault_Code_Type_2':str, 'Fault_Code_Type_3':str, 'Fault_Code_Type_4':str}\n",
    "\n",
    "df = pd.read_csv('sampledata.csv', sep=',', dtype=dtype, parse_dates=['Date'], infer_datetime_format=True)\n",
    "\n",
    "df = df.drop_duplicates(['DeviceID', 'Date'])\n",
    "\n",
    "\n",
    "df = df.dropna(how='any', subset=['DeviceID', 'Date'])\n",
    "\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Year'] = df['Date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_features = ['Date', 'Day', 'Month', 'Year']\n",
    "categorical_features = ['DeviceID', 'Categorical_1', 'Categorical_2', 'Categorical_3', 'Categorical_4',\n",
    "                        'Fault_Code_Type_1', 'Fault_Code_Type_2', 'Fault_Code_Type_3', 'Fault_Code_Type_4']\n",
    "warning_type1_features = [feature for feature in df.columns if feature.startswith('Warning_1')]\n",
    "warning_type2_features = [feature for feature in df.columns if feature.startswith('Warning_2')]\n",
    "warning_features = warning_type1_features + warning_type2_features\n",
    "numeric_features = list(set(df.columns) - set(datetime_features) - set(categorical_features))\n",
    "\n",
    "values = dict(zip(numeric_features, np.zeros(len(numeric_features))))\n",
    "df = df.fillna(value=values)\n",
    "\n",
    "values = dict(zip(categorical_features, ['Unknown' for k in range(len(categorical_features))]))\n",
    "df = df.fillna(value=values)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numeric_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in numeric_features:\n",
    "    df.loc[(df[feature] < 0), feature] = 0\n",
    "\n",
    "df[numeric_features].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExploratoryDataAnalysis(df, device, feature, start_date, end_date):\n",
    "    if(device != None and feature != None):\n",
    "        eda = df.loc[(df['DeviceID'] == device)]\n",
    "\n",
    "        from_date = start_date if start_date != None else FIRST_DATE\n",
    "        to_date = end_date if end_date != None else LAST_DATE\n",
    "\n",
    "        if(pd.date_range(from_date, to_date).size > 0):\n",
    "            eda = eda[eda['Date'].isin(pd.date_range(from_date, to_date))]\n",
    "\n",
    "            eda.plot(x='Date', y=feature)\n",
    "\n",
    "def f(device, feature, start_date, end_date):\n",
    "    ExploratoryDataAnalysis(df, device, feature, start_date, end_date)\n",
    "\n",
    "deviceID_selector = widgets.Dropdown(options=df.DeviceID.unique(), value=None, description='Device ID:', disabled=False)\n",
    "feature_selector = widgets.Dropdown(options=numeric_features, value='Usage_Count_1', description='Feature:', disabled=False)\n",
    "start_date_selector = widgets.DatePicker(value=FIRST_DATE, description='From:', disabled=False)\n",
    "end_date_selector = widgets.DatePicker(value=LAST_DATE, description='To:', disabled=False)\n",
    "\n",
    "w = widgets.interactive(f, device=deviceID_selector, feature=feature_selector,\n",
    "                        start_date=start_date_selector, end_date=end_date_selector)\n",
    "\n",
    "left_box = widgets.VBox([w.children[0], w.children[1]])\n",
    "right_box = widgets.VBox([w.children[2], w.children[3]])\n",
    "controls = widgets.HBox([left_box, right_box])\n",
    "output = w.children[-1]\n",
    "display(widgets.VBox([controls, output]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExploratoryDataAnalysis2(df, device, feature_x, feature_y, start_date, end_date):\n",
    "    if(device != None and feature_x != None and feature_y != None):\n",
    "        eda = df.loc[(df['DeviceID'] == device)]\n",
    "\n",
    "        from_date = start_date if start_date != None else FIRST_DATE\n",
    "        to_date = end_date if end_date != None else LAST_DATE\n",
    "\n",
    "        if(pd.date_range(from_date, to_date).size > 0):\n",
    "            eda = eda[eda['Date'].isin(pd.date_range(from_date, to_date))]\n",
    "\n",
    "            eda.plot.scatter(x=feature_x, y=feature_y)\n",
    "\n",
    "def f2(device, feature_x, feature_y, start_date, end_date):\n",
    "    ExploratoryDataAnalysis2(df, device, feature_x, feature_y, start_date, end_date)\n",
    "\n",
    "deviceID_selector = widgets.Dropdown(options=df.DeviceID.unique(), value=None, description='Device ID:', disabled=False)\n",
    "feature_x_selector = widgets.Dropdown(options=numeric_features, value='Usage_Count_1', description='X-Feature:', disabled=False)\n",
    "feature_y_selector = widgets.Dropdown(options=numeric_features, value='Usage_Count_2', description='Y-Feature:', disabled=False)\n",
    "start_date_selector = widgets.DatePicker(value=FIRST_DATE, description='From:', disabled=False)\n",
    "end_date_selector = widgets.DatePicker(value=LAST_DATE, description='To:', disabled=False)\n",
    "\n",
    "w = widgets.interactive(f2, device=deviceID_selector, feature_x=feature_x_selector, feature_y=feature_y_selector,\n",
    "                        start_date=start_date_selector, end_date=end_date_selector)\n",
    "\n",
    "left_box = widgets.VBox([w.children[0]])\n",
    "center_box = widgets.VBox([w.children[1], w.children[2]])\n",
    "right_box = widgets.VBox([w.children[3], w.children[4]])\n",
    "controls = widgets.HBox([left_box, center_box, right_box])\n",
    "output = w.children[-1]\n",
    "display(widgets.VBox([controls, output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExploratoryDataAnalysis3(df, feature):\n",
    "    eda = pd.pivot_table(df, values=feature, index='Year', columns='Month', aggfunc=np.sum)\n",
    "    plt.figure(figsize = (16,4))\n",
    "    plt.title('Cumulative Feature by Month and Year', fontsize=20)\n",
    "    sns.heatmap(eda, cmap=\"YlGnBu\", annot=True, fmt='.0f')\n",
    "\n",
    "def f3(feature):\n",
    "    ExploratoryDataAnalysis3(df, feature)\n",
    "\n",
    "feature_selector = widgets.Dropdown(options=numeric_features, value='ProblemReported', description='Feature:', disabled=False)\n",
    "\n",
    "w = widgets.interactive(f3, feature=feature_selector)\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df[['Error_Count_1', 'Error_Count_2', 'Error_Count_3', 'Error_Count_4',\n",
    "              'Error_Count_5', 'Error_Count_6', 'Error_Count_7', 'Error_Count_8', 'ProblemReported',\n",
    "            'Problem_Type_1', 'Problem_Type_2', 'Problem_Type_3', 'Problem_Type_4', 'Usage_Count_1', 'Usage_Count_2']]\n",
    "corr = df_corr.corr()\n",
    "plt.figure(figsize = (16,4))\n",
    "plt.title('Correlation matrix between numerical features', fontsize=20)\n",
    "sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, cmap=\"YlGnBu\", annot=True, fmt='.2f')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Warning_Type_1'] = sum(df[feature] for feature in warning_type1_features)\n",
    "df['Warning_Type_2'] = sum(df[feature] for feature in warning_type2_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pt1 = pd.pivot_table(df, values=['Categorical_1', 'Categorical_2', 'Categorical_3', 'Categorical_4'],\n",
    "#                     columns='DeviceID', aggfunc=np.std)\n",
    "#plt.figure(figsize = (16,4))\n",
    "#sns.heatmap(pt1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_cat = df[['Categorical_1', 'Categorical_2', 'Categorical_3', 'Categorical_4']].describe()\n",
    "df_cat.round(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Categorical_1_Grouped'] = pd.qcut(df['Categorical_1'], 4, labels=['LL', 'L', 'H', 'HH'])\n",
    "df['Categorical_2_Grouped'] = pd.qcut(df['Categorical_2'], 4, labels=['LL', 'L', 'H', 'HH'])\n",
    "df['Categorical_3_Grouped'] = pd.qcut(df['Categorical_3'], 3, labels=['LL', 'L', 'H'])\n",
    "df['Categorical_4_Grouped'] = pd.qcut(df['Categorical_4'], 4, labels=['LL', 'L', 'H', 'HH'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df[['Categorical_1_Grouped', 'Categorical_2_Grouped', 'Categorical_3_Grouped', 'Categorical_4_Grouped']].describe()\n",
    "df_cat.round(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pbm = df[['Problem_Type_1', 'Problem_Type_2', 'Problem_Type_3', 'Problem_Type_4']].describe()\n",
    "df_pbm.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_err = df[['Error_Count_1', 'Error_Count_2', 'Error_Count_3', 'Error_Count_4',\n",
    "            'Error_Count_5', 'Error_Count_6', 'Error_Count_7', 'Error_Count_8']].describe()\n",
    "df_err.round(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Error_Count_5'] = [1 if x > 1 else x for x in df['Error_Count_5']]\n",
    "df[['Error_Count_5']].describe().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flt = df[['Fault_Code_Type_1', 'Fault_Code_Type_2', 'Fault_Code_Type_3', 'Fault_Code_Type_4']].describe()\n",
    "df_flt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fault_Code_Type_1_Count'] = [0 if x == 'Unknown' else 1 for x in df['Fault_Code_Type_1']]\n",
    "df['Fault_Code_Type_2_Count'] = [0 if x == 'Unknown' else 1 for x in df['Fault_Code_Type_2']]\n",
    "df['Fault_Code_Type_3_Count'] = [0 if x == 'Unknown' else 1 for x in df['Fault_Code_Type_3']]\n",
    "df['Fault_Code_Type_4_Count'] = [0 if x == 'Unknown' else 1 for x in df['Fault_Code_Type_4']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Problem_Type_1_Per_Usage_1'] = np.where(df['Usage_Count_1'] == 0, 0, df['Problem_Type_1'] / df['Usage_Count_1'])\n",
    "df['Problem_Type_2_Per_Usage_1'] = np.where(df['Usage_Count_1'] == 0, 0, df['Problem_Type_2'] / df['Usage_Count_1'])\n",
    "df['Problem_Type_3_Per_Usage_1'] = np.where(df['Usage_Count_1'] == 0, 0, df['Problem_Type_3'] / df['Usage_Count_1'])\n",
    "df['Problem_Type_4_Per_Usage_1'] = np.where(df['Usage_Count_1'] == 0, 0, df['Problem_Type_4'] / df['Usage_Count_1'])\n",
    "df['Problem_Type_1_Per_Usage_2'] = np.where(df['Usage_Count_2'] == 0, 0, df['Problem_Type_1'] / df['Usage_Count_2'])\n",
    "df['Problem_Type_2_Per_Usage_2'] = np.where(df['Usage_Count_2'] == 0, 0, df['Problem_Type_2'] / df['Usage_Count_2'])\n",
    "df['Problem_Type_3_Per_Usage_2'] = np.where(df['Usage_Count_2'] == 0, 0, df['Problem_Type_3'] / df['Usage_Count_2'])\n",
    "df['Problem_Type_4_Per_Usage_2'] = np.where(df['Usage_Count_2'] == 0, 0, df['Problem_Type_4'] / df['Usage_Count_2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fault_Code_Type_1_Per_Usage_1'] = np.where(df['Usage_Count_1'] == 0, 0, df['Fault_Code_Type_1_Count'] / df['Usage_Count_1'])\n",
    "df['Fault_Code_Type_2_Per_Usage_1'] = np.where(df['Usage_Count_1'] == 0, 0, df['Fault_Code_Type_2_Count'] / df['Usage_Count_1'])\n",
    "df['Fault_Code_Type_3_Per_Usage_1'] = np.where(df['Usage_Count_1'] == 0, 0, df['Fault_Code_Type_3_Count'] / df['Usage_Count_1'])\n",
    "df['Fault_Code_Type_4_Per_Usage_1'] = np.where(df['Usage_Count_1'] == 0, 0, df['Fault_Code_Type_4_Count'] / df['Usage_Count_1'])\n",
    "df['Fault_Code_Type_1_Per_Usage_2'] = np.where(df['Usage_Count_2'] == 0, 0, df['Fault_Code_Type_1_Count'] / df['Usage_Count_2'])\n",
    "df['Fault_Code_Type_2_Per_Usage_2'] = np.where(df['Usage_Count_2'] == 0, 0, df['Fault_Code_Type_2_Count'] / df['Usage_Count_2'])\n",
    "df['Fault_Code_Type_3_Per_Usage_2'] = np.where(df['Usage_Count_2'] == 0, 0, df['Fault_Code_Type_3_Count'] / df['Usage_Count_2'])\n",
    "df['Fault_Code_Type_4_Per_Usage_2'] = np.where(df['Usage_Count_2'] == 0, 0, df['Fault_Code_Type_4_Count'] / df['Usage_Count_2'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Warning_Type_1_Per_Usage_1'] = np.where(df['Usage_Count_1'] == 0, 0, df['Warning_Type_1'] / df['Usage_Count_1'])\n",
    "df['Warning_Type_2_Per_Usage_1'] = np.where(df['Usage_Count_1'] == 0, 0, df['Warning_Type_2'] / df['Usage_Count_1'])\n",
    "df['Warning_Type_1_Per_Usage_2'] = np.where(df['Usage_Count_2'] == 0, 0, df['Warning_Type_1'] / df['Usage_Count_2'])\n",
    "df['Warning_Type_2_Per_Usage_2'] = np.where(df['Usage_Count_2'] == 0, 0, df['Warning_Type_2'] / df['Usage_Count_2'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_features = [feature for feature in df.columns if 'Per_Usage' in feature]\n",
    "\n",
    "for feature in ratio_features:\n",
    "    df[feature + '_Grouped'] = ['>1' if x > 1 else '>0' if x > 0 else 'Zero' for x in df[feature]]\n",
    "\n",
    "features_grouped = [feature for feature in df.columns if '_Grouped' in feature]\n",
    "features_grouped.extend(['Error_Count_1', 'Error_Count_2', 'Error_Count_3', 'Error_Count_4',\n",
    "                         'Error_Count_5', 'Error_Count_6', 'Error_Count_7', 'Error_Count_8'])\n",
    "df_grp = df[features_grouped]\n",
    "df_grp.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grp_ohe = pd.DataFrame()\n",
    "\n",
    "for feature in df_grp.columns:\n",
    "    temp = pd.get_dummies(df_grp[feature], prefix=feature)\n",
    "    df_grp_ohe[temp.columns] = temp\n",
    "\n",
    "df_grp_ohe.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([feature for feature in df.columns if '_Grouped' in feature], axis=1)\n",
    "\n",
    "df = pd.concat([df, df_grp_ohe], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for feature in ratio_features:\n",
    "    df[feature] = scaler.fit_transform(np.array(df[feature]).reshape(-1, 1))\n",
    "\n",
    "df[[feature for feature in ratio_features]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ae_t1_u1 = pd.DataFrame()\n",
    "df_ae_t1_u2 = pd.DataFrame()\n",
    "df_ae_t2_u1 = pd.DataFrame()\n",
    "df_ae_t2_u2 = pd.DataFrame()\n",
    "\n",
    "for feature in warning_type1_features:\n",
    "    df_ae_t1_u1[feature + '_Per_Usage_1'] = np.where(df['Usage_Count_1'] == 0, 0, df[feature] / df['Usage_Count_1'])\n",
    "    df_ae_t1_u2[feature + '_Per_Usage_2'] = np.where(df['Usage_Count_2'] == 0, 0, df[feature] / df['Usage_Count_2'])\n",
    "\n",
    "for feature in warning_type2_features:\n",
    "    df_ae_t2_u1[feature + '_Per_Usage_1'] = np.where(df['Usage_Count_1'] == 0, 0, df[feature] / df['Usage_Count_1'])\n",
    "    df_ae_t2_u2[feature + '_Per_Usage_2'] = np.where(df['Usage_Count_2'] == 0, 0, df[feature] / df['Usage_Count_2'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "for column in df_ae_t1_u1.columns:\n",
    "    df_ae_t1_u1[column] = scaler.fit_transform(np.array(df_ae_t1_u1[column]).reshape(-1, 1))\n",
    "\n",
    "for column in df_ae_t1_u2.columns:\n",
    "    df_ae_t1_u2[column] = scaler.fit_transform(np.array(df_ae_t1_u2[column]).reshape(-1, 1))\n",
    "\n",
    "for column in df_ae_t2_u1.columns:\n",
    "    df_ae_t2_u1[column] = scaler.fit_transform(np.array(df_ae_t2_u1[column]).reshape(-1, 1))\n",
    "\n",
    "for column in df_ae_t2_u2.columns:\n",
    "    df_ae_t2_u2[column] = scaler.fit_transform(np.array(df_ae_t2_u2[column]).reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "seed = 2019\n",
    "np.random.seed(seed)\n",
    "AE_FEATURES_NUM = 5\n",
    "\n",
    "entry = Input(shape=(df_ae_t1_u1.shape[1],))\n",
    "coder = Dense(AE_FEATURES_NUM, activation='relu', name='intermediate')(entry)\n",
    "decoder = Dense(df_ae_t1_u1.shape[1], activation='relu')(coder)\n",
    "autoencoder = Model(entry, decoder)\n",
    "\n",
    "encoder1 = Model(autoencoder.input, autoencoder.get_layer('intermediate').output)\n",
    "\n",
    "autoencoder.compile(optimizer='nadam', loss='mse', metrics=['accuracy'])\n",
    "autoencoder.fit(df_ae_t1_u1, df_ae_t1_u1, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = Input(shape=(df_ae_t1_u2.shape[1],))\n",
    "coder = Dense(AE_FEATURES_NUM, activation='relu', name='intermediate')(entry)\n",
    "decoder = Dense(df_ae_t1_u2.shape[1], activation='relu')(coder)\n",
    "autoencoder = Model(entry, decoder)\n",
    "\n",
    "encoder2 = Model(autoencoder.input, autoencoder.get_layer('intermediate').output)\n",
    "\n",
    "autoencoder.compile(optimizer='nadam', loss='mse', metrics=['accuracy'])\n",
    "autoencoder.fit(df_ae_t1_u2, df_ae_t1_u2, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = Input(shape=(df_ae_t2_u1.shape[1],))\n",
    "coder = Dense(AE_FEATURES_NUM, activation='relu', name='intermediate')(entry)\n",
    "decoder = Dense(df_ae_t2_u1.shape[1], activation='relu')(coder)\n",
    "autoencoder = Model(entry, decoder)\n",
    "\n",
    "encoder3 = Model(autoencoder.input, autoencoder.get_layer('intermediate').output)\n",
    "\n",
    "autoencoder.compile(optimizer='nadam', loss='mse', metrics=['accuracy'])\n",
    "autoencoder.fit(df_ae_t2_u1, df_ae_t2_u1, epochs=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = Input(shape=(df_ae_t2_u2.shape[1],))\n",
    "coder = Dense(AE_FEATURES_NUM, activation='relu', name='intermediate')(entry)\n",
    "decoder = Dense(df_ae_t2_u2.shape[1], activation='relu')(coder)\n",
    "autoencoder = Model(entry, decoder)\n",
    "\n",
    "encoder4 = Model(autoencoder.input, autoencoder.get_layer('intermediate').output)\n",
    "\n",
    "autoencoder.compile(optimizer='nadam', loss='mse', metrics=['accuracy'])\n",
    "autoencoder.fit(df_ae_t2_u2, df_ae_t2_u2, epochs=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([feature for feature in warning_features], axis=1)\n",
    "\n",
    "df_ae_t1_u1 = encoder1.predict(df_ae_t1_u1)\n",
    "df_ae_t1_u1 = pd.DataFrame(df_ae_t1_u1, columns=['AE_T1U1_Warning_' + str(i + 1) for i in range(AE_FEATURES_NUM)])\n",
    "\n",
    "df_ae_t1_u2 = encoder2.predict(df_ae_t1_u2)\n",
    "df_ae_t1_u2 = pd.DataFrame(df_ae_t1_u2, columns=['AE_T1U2_Warning_' + str(i + 1) for i in range(AE_FEATURES_NUM)])\n",
    "\n",
    "df_ae_t2_u1 = encoder3.predict(df_ae_t2_u1)\n",
    "df_ae_t2_u1 = pd.DataFrame(df_ae_t2_u1, columns=['AE_T2U1_Warning_' + str(i + 1) for i in range(AE_FEATURES_NUM)])\n",
    "\n",
    "df_ae_t2_u2 = encoder4.predict(df_ae_t2_u2)\n",
    "df_ae_t2_u2 = pd.DataFrame(df_ae_t2_u2, columns=['AE_T2U2_Warning_' + str(i + 1) for i in range(AE_FEATURES_NUM)])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "for column in df_ae_t1_u1.columns:\n",
    "    df_ae_t1_u1[column] = scaler.fit_transform(np.array(df_ae_t1_u1[column]).reshape(-1, 1))\n",
    "\n",
    "for column in df_ae_t1_u2.columns:\n",
    "    df_ae_t1_u2[column] = scaler.fit_transform(np.array(df_ae_t1_u2[column]).reshape(-1, 1))\n",
    "\n",
    "for column in df_ae_t2_u1.columns:\n",
    "    df_ae_t2_u1[column] = scaler.fit_transform(np.array(df_ae_t2_u1[column]).reshape(-1, 1))\n",
    "\n",
    "for column in df_ae_t2_u2.columns:\n",
    "    df_ae_t2_u2[column] = scaler.fit_transform(np.array(df_ae_t2_u2[column]).reshape(-1, 1))\n",
    "\n",
    "df = pd.concat([df, df_ae_t1_u1, df_ae_t1_u2, df_ae_t2_u1, df_ae_t2_u2], axis=1)\n",
    "\n",
    "\n",
    "df.to_pickle('sampledata2.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#save the model\n",
    "model.save('my_trained_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing data\n",
    "model = tf.keras.models.load_model('my_trained_model.keras')\n",
    "\n",
    "test_data = pd.read_csv('Testingdata01.csv')\n",
    "\n",
    "test_data_np = test_data.to_numpy()\n",
    "\n",
    "predictions = model.predict(test_data_np)\n",
    "\n",
    "reconstruction_errors = np.mean(np.square(test_data_np - predictions), axis=1)\n",
    "\n",
    "threshold = 0.2\n",
    "\n",
    "anomalies_indices = np.where(reconstruction_errors > threshold)[0]\n",
    "print(\"Anomalies detected at indices:\", anomalies_indices)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(reconstruction_errors, marker='o', linestyle='', color='r', label='Reconstruction Errors')\n",
    "plt.axhline(y=threshold, color='b', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Errors vs Data Indices')\n",
    "plt.xlabel('Data Indices')\n",
    "plt.ylabel('Reconstruction Errors')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(reconstruction_errors, color='r', label='Reconstruction Errors')\n",
    "plt.axhline(y=threshold, color='b', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Errors Over Data Samples')\n",
    "plt.xlabel('Data Samples')\n",
    "plt.ylabel('Reconstruction Errors')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('sampledata2.pkl')\n",
    "ROLLING_DAYS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for device in df.DeviceID.unique():\n",
    "    reported_array = df.loc[(df.DeviceID == device), 'ProblemReported'].values\n",
    "    reported_array = reported_array[::-1]\n",
    "    problem_array = np.zeros(len(reported_array))\n",
    "    daysto_array = np.zeros(len(reported_array))\n",
    "    problem_found = False\n",
    "\n",
    "    for i in range(len(reported_array)):\n",
    "\n",
    "        if not problem_found:\n",
    "            daysto_array[i] = -1\n",
    "        else:\n",
    "            daysto_array[i] = daysto_array[i - 1] + 1\n",
    "\n",
    "        if reported_array[i] == 1:\n",
    "            problem_found = True\n",
    "            problem_array[i] = 1\n",
    "            daysto_array[i] = 0\n",
    "\n",
    "            for j in range(i + 1, i + ROLLING_DAYS):\n",
    "                if j < len(reported_array):\n",
    "                    problem_array[j] = 1\n",
    "\n",
    "    problem_array = problem_array[::-1]\n",
    "    df.loc[(df.DeviceID == device), 'Problem'] = problem_array\n",
    "\n",
    "    daysto_array = daysto_array[::-1]\n",
    "    df.loc[(df.DeviceID == device), 'DaysTo'] = daysto_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_anomalies = df.loc[(df.DaysTo >= 540)]\n",
    "print(train_no_anomalies.shape)\n",
    "\n",
    "test_no_anomalies = df.loc[(df.DaysTo >= 530) & (df.DaysTo < 540)]\n",
    "print(test_no_anomalies.shape)\n",
    "\n",
    "test_anomalies = df.loc[(df.ProblemReported == 1)]\n",
    "print(test_anomalies.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = pd.concat([train_no_anomalies, test_no_anomalies, test_anomalies], ignore_index=True)\n",
    "low_std = {}\n",
    "\n",
    "for column in df_A.columns:\n",
    "    try:\n",
    "        if df_A[column].std() < 0.01:\n",
    "            low_std[column] = df_A[column].std()\n",
    "    except:\n",
    "        None\n",
    "df_A = df_A.drop([k for k in low_std.keys()], axis=1)\n",
    "\n",
    "features = ['ProblemReported', 'DaysTo']\n",
    "features.extend([feature for feature in df_A.columns if 'Error_' in feature and len(feature) == 13])\n",
    "features.extend([feature for feature in df_A.columns if 'Fault_' in feature and len(feature) == 23])\n",
    "features.extend([feature for feature in df_A.columns if 'Problem_' in feature and len(feature) == 26])\n",
    "features.extend([feature for feature in df_A.columns if 'Warning_' in feature and len(feature) == 26])\n",
    "features.extend([feature for feature in df_A.columns if 'AE_' in feature and len(feature) == 17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = df_A[features]\n",
    "print(df_A.shape)\n",
    "\n",
    "train_no_anomalies = df_A.loc[(df_A.DaysTo >= 540)]\n",
    "train_no_anomalies = train_no_anomalies.drop(['ProblemReported', 'DaysTo'], axis=1)\n",
    "print(train_no_anomalies.shape)\n",
    "\n",
    "test_no_anomalies = df_A.loc[(df_A.DaysTo >= 530) & (df_A.DaysTo < 540)]\n",
    "test_no_anomalies = test_no_anomalies.drop(['ProblemReported', 'DaysTo'], axis=1)\n",
    "print(test_no_anomalies.shape)\n",
    "\n",
    "test_anomalies = df_A.loc[(df_A.ProblemReported == 1)]\n",
    "test_anomalies = test_anomalies.drop(['ProblemReported', 'DaysTo'], axis=1)\n",
    "print(test_anomalies.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cb4cdeafc61a6c89e92f7723c86672e1ce8acb7f4f325f2b080f67667d6c07a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
